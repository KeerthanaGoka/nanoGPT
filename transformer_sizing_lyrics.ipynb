{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenizer"
      ],
      "metadata": {
        "id": "N46SmVlGMHFO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwrs2QmKbQJo"
      },
      "outputs": [],
      "source": [
        "### Word Tokenizer- Vocab size=50304\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "block_size = 64\n",
        "vocab_size = 50304\n",
        "n_layer = 4\n",
        "n_head = 4\n",
        "n_embd = 128\n",
        "bias = False\n",
        "assert not bias, \"this notebook assumes bias=False just for simplicity\"\n",
        "\n",
        "def params():\n",
        "    \"\"\" estimates the number of parameters in the model\"\"\"\n",
        "    out = OrderedDict()\n",
        "\n",
        "    # token and position embeddings\n",
        "    out['emebedding/position'] = n_embd * block_size\n",
        "    out['embedding/token'] = n_embd * vocab_size\n",
        "    out['embedding'] = out['emebedding/position'] + out['embedding/token']\n",
        "\n",
        "    # attention blocks\n",
        "    out['attention/ln'] = n_embd # note, bias=False in our LN\n",
        "    out['attention/kqv'] = n_embd * 3*n_embd\n",
        "    out['attention/proj'] = n_embd**2\n",
        "    out['attention'] = out['attention/ln'] + out['attention/kqv'] + out['attention/proj']\n",
        "\n",
        "    # MLP blocks\n",
        "    ffw_size = 4*n_embd # feed forward size\n",
        "    out['mlp/ln'] = n_embd\n",
        "    out['mlp/ffw'] = n_embd * ffw_size\n",
        "    out['mlp/proj'] = ffw_size * n_embd\n",
        "    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n",
        "\n",
        "    # the transformer and the rest of it\n",
        "    out['block'] = out['attention'] + out['mlp']\n",
        "    out['transformer'] = n_layer * out['block']\n",
        "    out['ln_f'] = n_embd # final layernorm\n",
        "    out['dense'] = 0 # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
        "\n",
        "    # total\n",
        "    out['total'] = out['embedding'] + out['transformer'] + out['ln_f'] + out['dense']\n",
        "\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare our param count to that reported by PyTorch\n",
        "p = params()\n",
        "params_total = p['total']\n",
        "print(f\"we see: {params_total}, expected: {(7233536+1152)}, match: {params_total == (7233536+1152)}\")\n",
        "# create a header\n",
        "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
        "for k,v in p.items():\n",
        "    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtuGl7LGcZfr",
        "outputId": "85115fc3-0ca5-4aeb-99b6-0b0a4a218200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we see: 7234688, expected: 7234688, match: True\n",
            "name                 params     ratio (%) \n",
            "emebedding/position        8192     0.1132\n",
            "embedding/token         6438912    89.0005\n",
            "embedding               6447104    89.1138\n",
            "attention/ln                128     0.0018\n",
            "attention/kqv             49152     0.6794\n",
            "attention/proj            16384     0.2265\n",
            "attention                 65664     0.9076\n",
            "mlp/ln                      128     0.0018\n",
            "mlp/ffw                   65536     0.9059\n",
            "mlp/proj                  65536     0.9059\n",
            "mlp                      131200     1.8135\n",
            "block                    196864     2.7211\n",
            "transformer              787456    10.8845\n",
            "ln_f                        128     0.0018\n",
            "dense                         0     0.0000\n",
            "total                   7234688   100.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character Tokenizer"
      ],
      "metadata": {
        "id": "ozBanUNfMVJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Character Tokenizer- Vocab size=77\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "block_size = 64\n",
        "vocab_size = 77\n",
        "n_layer = 4\n",
        "n_head = 4\n",
        "n_embd = 128\n",
        "bias = False\n",
        "assert not bias, \"this notebook assumes bias=False just for simplicity\"\n",
        "\n",
        "def params():\n",
        "    \"\"\" estimates the number of parameters in the model\"\"\"\n",
        "    out = OrderedDict()\n",
        "\n",
        "    # token and position embeddings\n",
        "    out['emebedding/position'] = n_embd * block_size\n",
        "    out['embedding/token'] = n_embd * vocab_size\n",
        "    out['embedding'] = out['emebedding/position'] + out['embedding/token']\n",
        "\n",
        "    # attention blocks\n",
        "    out['attention/ln'] = n_embd # note, bias=False in our LN\n",
        "    out['attention/kqv'] = n_embd * 3*n_embd\n",
        "    out['attention/proj'] = n_embd**2\n",
        "    out['attention'] = out['attention/ln'] + out['attention/kqv'] + out['attention/proj']\n",
        "\n",
        "    # MLP blocks\n",
        "    ffw_size = 4*n_embd # feed forward size\n",
        "    out['mlp/ln'] = n_embd\n",
        "    out['mlp/ffw'] = n_embd * ffw_size\n",
        "    out['mlp/proj'] = ffw_size * n_embd\n",
        "    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n",
        "\n",
        "    # the transformer and the rest of it\n",
        "    out['block'] = out['attention'] + out['mlp']\n",
        "    out['transformer'] = n_layer * out['block']\n",
        "    out['ln_f'] = n_embd # final layernorm\n",
        "    out['dense'] = 0 # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
        "\n",
        "    # total\n",
        "    out['total'] = out['embedding'] + out['transformer'] + out['ln_f'] + out['dense']\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "U76UuLk2b-vG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare our param count to that reported by PyTorch\n",
        "p = params()\n",
        "params_total = p['total']\n",
        "print(f\"we see: {params_total}, expected: {(804480+1152)}, match: {params_total == (804480+1152)}\")\n",
        "# create a header\n",
        "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
        "for k,v in p.items():\n",
        "    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUJVq0-UMbeV",
        "outputId": "04372c5b-71d1-4354-c663-44d2fde80194"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we see: 805632, expected: 805632, match: True\n",
            "name                 params     ratio (%) \n",
            "emebedding/position        8192     1.0168\n",
            "embedding/token            9856     1.2234\n",
            "embedding                 18048     2.2402\n",
            "attention/ln                128     0.0159\n",
            "attention/kqv             49152     6.1010\n",
            "attention/proj            16384     2.0337\n",
            "attention                 65664     8.1506\n",
            "mlp/ln                      128     0.0159\n",
            "mlp/ffw                   65536     8.1347\n",
            "mlp/proj                  65536     8.1347\n",
            "mlp                      131200    16.2854\n",
            "block                    196864    24.4360\n",
            "transformer              787456    97.7439\n",
            "ln_f                        128     0.0159\n",
            "dense                         0     0.0000\n",
            "total                    805632   100.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lwNxQvMM2fd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}